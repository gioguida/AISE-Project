{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66a19e12",
   "metadata": {},
   "source": [
    "# Task 3: All2All Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8344e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a906d8",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b345ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNO_bn import FNO1d_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfe10e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c02f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: 0\n",
      "CUDA device name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "CUDA memory allocated: 0.00 MB\n",
      "CUDA memory reserved: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Verify CUDA setup\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"CUDA memory reserved: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5949f60",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ad7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1ad7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1024 # number of training samples\n",
    "\n",
    "# train dataset shape: (1024, 5, 128)\n",
    "# 1024: number of trajectories\n",
    "# 5: time snapthots of the solution: t= 0, 0.25, 0.5, 0.75, 1.0\n",
    "# 128: spatial resolution of the data\n",
    "\n",
    "train_dataset = torch.from_numpy(np.load(\"data/data_train_128.npy\")).type(torch.float32)\n",
    "test_dataset = torch.from_numpy(np.load(\"data/data_test_128.npy\")).type(torch.float32)\n",
    "# add time as input feature\n",
    "time_train = torch.linspace(0, 1, train_dataset.shape[1]).reshape(1, -1, 1)\n",
    "time_test = torch.linspace(0, 1, test_dataset.shape[1]).reshape(1, -1, 1)\n",
    "train_dataset = torch.cat([train_dataset, time_train.repeat(train_dataset.shape[0], 1, 1)], dim=-1)\n",
    "test_dataset = torch.cat([test_dataset, time_test.repeat(test_dataset.shape[0], 1, 1)], dim=-1)\n",
    "\n",
    "# add grid coordinates as input feature\n",
    "grid_train = torch.linspace(0, 1, train_dataset.shape[2]).reshape(1, 1, -1)\n",
    "grid_test = torch.linspace(0, 1, test_dataset.shape[2]).reshape(1, 1, -1)\n",
    "train_dataset = torch.cat([train_dataset, grid_train.repeat(train_dataset.shape[0], train_dataset.shape[1], 1)], dim=-1)\n",
    "test_dataset = torch.cat([test_dataset, grid_test.repeat(test_dataset.shape[0], test_dataset.shape[1], 1)], dim=-1)\n",
    "\n",
    "# Move data to device\n",
    "train_dataset = train_dataset.to(device)\n",
    "test_dataset = test_dataset.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(TensorDataset(train_dataset), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_dataset), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91cbe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDEDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 which=\"training\",\n",
    "                 training_samples = 256,\n",
    "                 resolution = 128,\n",
    "                 device='cpu'):\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.device = device\n",
    "        self.data = np.load(f\"data/data_train_{resolution}.npy\")\n",
    "\n",
    "        self.T = 5\n",
    "        # Precompute all possible (t_initial, t_final) pairs within the specified range.\n",
    "        self.time_pairs = [(i, j) for i in range(0, self.T) for j in range(i + 1, self.T)]\n",
    "        self.len_times  = len(self.time_pairs)\n",
    "\n",
    "        # Total samples available in the dataset\n",
    "        total_samples = self.data.shape[0]\n",
    "        self.n_val = 32\n",
    "        self.n_test = 32\n",
    "\n",
    "        if which == \"training\":\n",
    "            self.length = training_samples * self.len_times\n",
    "            self.start_sample = 0\n",
    "        elif which == \"validation\":\n",
    "            self.length = self.n_val * self.len_times\n",
    "            self.start_sample = total_samples - self.n_val - self.n_test\n",
    "        elif which == \"test\":\n",
    "            self.length = self.n_test * self.len_times\n",
    "            self.start_sample = total_samples - self.n_test\n",
    "\n",
    "        self.mean = 0.018484\n",
    "        self.std  = 0.685405\n",
    "        \n",
    "        # Pre-create grid to avoid recreating it each time\n",
    "        self.grid = torch.linspace(0, 1, 128, dtype=torch.float32).reshape(128, 1).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_idx = self.start_sample + index // self.len_times\n",
    "        time_pair_idx = index % self.len_times\n",
    "        t_inp, t_out = self.time_pairs[time_pair_idx]\n",
    "        time = torch.tensor((t_out - t_inp)/4. + float(np.random.rand(1)[0]/10**6), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        inputs = torch.from_numpy(self.data[sample_idx, t_inp]).type(torch.float32).reshape(128, 1).to(self.device)\n",
    "        inputs = (inputs - self.mean)/self.std #Normalize\n",
    "        \n",
    "        # Add grid coordinates (already on correct device and dtype)\n",
    "        inputs = torch.cat((inputs, self.grid), dim=-1)  # (128, 2)\n",
    "\n",
    "        outputs = torch.from_numpy(self.data[sample_idx, t_out]).type(torch.float32).reshape(128).to(self.device)\n",
    "        outputs = (outputs - self.mean)/self.std #Normalize\n",
    "\n",
    "        return time, inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1738bad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1024, 5, 128)\n",
      "Training data mean: 0.018484\n",
      "Training data std: 0.685405\n",
      "Training data min: -3.095698\n",
      "Training data max: 3.086819\n",
      "\n",
      "Currently using:\n",
      "  mean = 0\n",
      "  std = 0.3835\n"
     ]
    }
   ],
   "source": [
    "# Check actual data statistics\n",
    "train_data = np.load(\"data/data_train_128.npy\")\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training data mean: {train_data.mean():.6f}\")\n",
    "print(f\"Training data std: {train_data.std():.6f}\")\n",
    "print(f\"Training data min: {train_data.min():.6f}\")\n",
    "print(f\"Training data max: {train_data.max():.6f}\")\n",
    "print(f\"\\nCurrently using:\")\n",
    "print(f\"  mean = 0\")\n",
    "print(f\"  std = 0.3835\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2f0b0",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987425ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1024 # Number of TRAJECTORIES for training\n",
    "batch_size = 128\n",
    "\n",
    "training_set = DataLoader(PDEDataset(\"training\", n_train, device=device), batch_size=batch_size, shuffle=True)\n",
    "testing_set = DataLoader(PDEDataset(\"validation\", device=device), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 12\n",
    "width = 64 # 64\n",
    "fno = FNO1d_bn(modes, width).to(device)  # model\n",
    "\n",
    "optimizer = torch.optim.Adam(fno.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',           # Minimize validation loss\n",
    "    factor=0.5,          # Multiply LR by 0.5\n",
    "    patience=5,          # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6         # Don't go below this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7333a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 0  ######### Train Loss: 0.069055861083325  ######### Relative L2 Test Norm: 30.784061431884766\n",
      "######### Epoch: 1  ######### Train Loss: 0.013917891695746221  ######### Relative L2 Test Norm: 25.36121940612793\n",
      "######### Epoch: 2  ######### Train Loss: 0.01021944034146145  ######### Relative L2 Test Norm: 23.08533541361491\n",
      "######### Epoch: 3  ######### Train Loss: 0.009151038801064715  ######### Relative L2 Test Norm: 21.494194984436035\n",
      "######### Epoch: 4  ######### Train Loss: 0.007858064035826829  ######### Relative L2 Test Norm: 18.88886610666911\n",
      "######### Epoch: 5  ######### Train Loss: 0.006972958079131786  ######### Relative L2 Test Norm: 18.218523025512695\n",
      "######### Epoch: 6  ######### Train Loss: 0.00868938402272761  ######### Relative L2 Test Norm: 20.032304445902508\n",
      "######### Epoch: 7  ######### Train Loss: 0.006984457839280367  ######### Relative L2 Test Norm: 17.835336685180664\n",
      "######### Epoch: 8  ######### Train Loss: 0.0074571165634552015  ######### Relative L2 Test Norm: 17.483575185139973\n",
      "######### Epoch: 9  ######### Train Loss: 0.004885968478629365  ######### Relative L2 Test Norm: 15.7942746480306\n",
      "######### Epoch: 10  ######### Train Loss: 0.004514343908522278  ######### Relative L2 Test Norm: 17.048800468444824\n",
      "######### Epoch: 11  ######### Train Loss: 0.0034798667606082746  ######### Relative L2 Test Norm: 16.463518460591633\n",
      "######### Epoch: 12  ######### Train Loss: 0.004214245709590614  ######### Relative L2 Test Norm: 17.048439661661785\n",
      "######### Epoch: 13  ######### Train Loss: 0.003944734415563289  ######### Relative L2 Test Norm: 17.687753359476726\n",
      "######### Epoch: 14  ######### Train Loss: 0.0032338582110241986  ######### Relative L2 Test Norm: 16.923936525980633\n",
      "######### Epoch: 15  ######### Train Loss: 0.0039249697263585405  ######### Relative L2 Test Norm: 22.454914093017578\n",
      "######### Epoch: 16  ######### Train Loss: 0.002428231992962537  ######### Relative L2 Test Norm: 17.177682876586914\n",
      "######### Epoch: 17  ######### Train Loss: 0.0018748188951576595  ######### Relative L2 Test Norm: 16.528746287027996\n",
      "######### Epoch: 18  ######### Train Loss: 0.0015641630147001707  ######### Relative L2 Test Norm: 17.98965867360433\n",
      "######### Epoch: 19  ######### Train Loss: 0.0012802144687157125  ######### Relative L2 Test Norm: 17.876378695170086\n",
      "######### Epoch: 20  ######### Train Loss: 0.001388248155126348  ######### Relative L2 Test Norm: 17.981417655944824\n",
      "######### Epoch: 21  ######### Train Loss: 0.0013596350836451165  ######### Relative L2 Test Norm: 17.41157817840576\n",
      "######### Epoch: 22  ######### Train Loss: 0.0009739689470734447  ######### Relative L2 Test Norm: 17.542235056559246\n",
      "######### Epoch: 23  ######### Train Loss: 0.0009147004333499353  ######### Relative L2 Test Norm: 17.924005190531414\n",
      "######### Epoch: 24  ######### Train Loss: 0.0008430813813902205  ######### Relative L2 Test Norm: 17.737485885620117\n",
      "######### Epoch: 25  ######### Train Loss: 0.0009342799683508928  ######### Relative L2 Test Norm: 17.456189791361492\n",
      "######### Epoch: 26  ######### Train Loss: 0.000824063202162506  ######### Relative L2 Test Norm: 18.535452842712402\n",
      "######### Epoch: 27  ######### Train Loss: 0.0008355563248187536  ######### Relative L2 Test Norm: 18.35296122233073\n",
      "######### Epoch: 28  ######### Train Loss: 0.0006237360805243953  ######### Relative L2 Test Norm: 18.30577023824056\n",
      "######### Epoch: 29  ######### Train Loss: 0.0006272554823226528  ######### Relative L2 Test Norm: 17.999927202860516\n",
      "######### Epoch: 30  ######### Train Loss: 0.0006434854334656847  ######### Relative L2 Test Norm: 18.73424180348714\n",
      "######### Epoch: 31  ######### Train Loss: 0.0006627912374824518  ######### Relative L2 Test Norm: 18.1222407023112\n",
      "######### Epoch: 32  ######### Train Loss: 0.000602653977148293  ######### Relative L2 Test Norm: 18.161390622456867\n",
      "######### Epoch: 33  ######### Train Loss: 0.0006464208971010521  ######### Relative L2 Test Norm: 18.596293131510418\n",
      "######### Epoch: 34  ######### Train Loss: 0.0005360657025448746  ######### Relative L2 Test Norm: 18.449816068013508\n",
      "######### Epoch: 35  ######### Train Loss: 0.0005940324903349392  ######### Relative L2 Test Norm: 18.426642735799152\n",
      "######### Epoch: 36  ######### Train Loss: 0.0005308830259309616  ######### Relative L2 Test Norm: 18.48211447397868\n",
      "######### Epoch: 37  ######### Train Loss: 0.0005250033174888813  ######### Relative L2 Test Norm: 18.788201014200848\n",
      "######### Epoch: 38  ######### Train Loss: 0.0005313850397214992  ######### Relative L2 Test Norm: 18.893014272054035\n",
      "######### Epoch: 39  ######### Train Loss: 0.0006836835074864212  ######### Relative L2 Test Norm: 18.809954325358074\n",
      "######### Epoch: 40  ######### Train Loss: 0.0005178437280846992  ######### Relative L2 Test Norm: 18.622056007385254\n",
      "######### Epoch: 41  ######### Train Loss: 0.00046912787547626067  ######### Relative L2 Test Norm: 18.750932057698567\n",
      "######### Epoch: 42  ######### Train Loss: 0.0005201200934607187  ######### Relative L2 Test Norm: 18.649831771850586\n",
      "######### Epoch: 43  ######### Train Loss: 0.00046714150194020475  ######### Relative L2 Test Norm: 18.507366180419922\n",
      "######### Epoch: 44  ######### Train Loss: 0.000539786514127627  ######### Relative L2 Test Norm: 18.640703519185383\n",
      "######### Epoch: 45  ######### Train Loss: 0.0005392537626903504  ######### Relative L2 Test Norm: 18.952803293863933\n",
      "######### Epoch: 46  ######### Train Loss: 0.0005034052062910632  ######### Relative L2 Test Norm: 18.864768346150715\n",
      "######### Epoch: 47  ######### Train Loss: 0.0004519517453445587  ######### Relative L2 Test Norm: 18.68715000152588\n",
      "######### Epoch: 48  ######### Train Loss: 0.0004708450504040229  ######### Relative L2 Test Norm: 18.636696815490723\n",
      "######### Epoch: 49  ######### Train Loss: 0.00045190773580543463  ######### Relative L2 Test Norm: 18.80460198720296\n"
     ]
    }
   ],
   "source": [
    "# Define the error function\n",
    "def relative_l2_error(pred, true):\n",
    "    diff_norm = torch.norm(pred - true, p=2, dim=1)\n",
    "    true_norm = torch.norm(true, p=2, dim=1)\n",
    "    return torch.mean(diff_norm / true_norm) * 100\n",
    "\n",
    "l = nn.MSELoss()  \n",
    "freq_print = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    fno.train()\n",
    "    train_mse = 0.0\n",
    "    for step, (time_batch, input_batch, output_batch) in enumerate(training_set):\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = fno(input_batch, time_batch).squeeze(-1)\n",
    "        loss_f = l(output_pred_batch, output_batch)\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "\n",
    "    # Validation with CORRECT L2 error\n",
    "    with torch.no_grad():\n",
    "        fno.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for step, (time_batch, input_batch, output_batch) in enumerate(testing_set):\n",
    "            output_pred_batch = fno(input_batch, time_batch).squeeze(-1)\n",
    "            # Use the correct relative L2 error\n",
    "            loss_f = relative_l2_error(output_pred_batch, output_batch)\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "    scheduler.step(test_relative_l2)\n",
    "\n",
    "\n",
    "    if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L2 Test Norm:\", test_relative_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253e70c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory before training: 47.62 MB\n",
      "Model on device: cuda:0\n",
      "Training data on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU memory before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory before training: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    print(f\"Model on device: {next(fno.parameters()).device}\")\n",
    "    print(f\"Training data on device: {next(iter(training_set))[1].device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7a9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "# torch.save(fno.state_dict(), \"fno1d_bn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7149826f",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5602cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 128\n",
      "Input shape: torch.Size([128, 128, 2])\n",
      "Time values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load test data at resolution 128\n",
    "test_data_raw = np.load(\"data/data_test_128.npy\")\n",
    "spatial_resolution = 128\n",
    "\n",
    "# Extract initial conditions (t=0) and final time solutions (t=1)\n",
    "initial_conditions = test_data_raw[:, 0, :]  # Shape: (n_samples, 128)\n",
    "final_time_ground_truth = test_data_raw[:, -1, :]  # Shape: (n_samples, 128)\n",
    "\n",
    "# Prepare initial conditions with grid coordinates\n",
    "grid = torch.linspace(0, 1, spatial_resolution, dtype=torch.float32).reshape(spatial_resolution, 1)\n",
    "initial_conditions_tensor = torch.from_numpy(initial_conditions).type(torch.float32).reshape(-1, spatial_resolution, 1)\n",
    "\n",
    "# Add grid coordinates to each sample\n",
    "initial_conditions_with_grid = torch.cat([initial_conditions_tensor, grid.repeat(initial_conditions_tensor.shape[0], 1, 1)], dim=-1)\n",
    "\n",
    "# Normalize initial conditions\n",
    "mean = 0.018484\n",
    "std = 0.685405\n",
    "initial_conditions_with_grid[:, :, 0] = (initial_conditions_with_grid[:, :, 0] - mean) / std\n",
    "\n",
    "# Prepare time tensor (full time span from 0 to 1)\n",
    "time_full = torch.ones(initial_conditions_with_grid.shape[0], dtype=torch.float32) * 1.0\n",
    "\n",
    "# Move to device\n",
    "initial_conditions_with_grid = initial_conditions_with_grid.to(device)\n",
    "time_full = time_full.to(device)\n",
    "\n",
    "print(f\"Test samples: {initial_conditions_with_grid.shape[0]}\")\n",
    "print(f\"Input shape: {initial_conditions_with_grid.shape}\")\n",
    "print(f\"Time values: {time_full[0].item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5f4c77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 Error: 38.5847%\n"
     ]
    }
   ],
   "source": [
    "# Test the model: predict from t=0 to t=1\n",
    "batch_size_test = 20\n",
    "n_test_samples = initial_conditions_with_grid.shape[0]\n",
    "\n",
    "all_predictions = []\n",
    "fno.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, n_test_samples, batch_size_test):\n",
    "        batch_end = min(i + batch_size_test, n_test_samples)\n",
    "        batch_input = initial_conditions_with_grid[i:batch_end]\n",
    "        batch_time = time_full[i:batch_end]\n",
    "        \n",
    "        # Predict final time\n",
    "        predictions = fno(batch_input, batch_time).squeeze(-1)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "all_predictions_denorm = all_predictions * std + mean\n",
    "\n",
    "# Convert ground truth to tensor\n",
    "final_time_ground_truth_tensor = torch.from_numpy(final_time_ground_truth).type(torch.float32).to(device)\n",
    "\n",
    "# Calculate relative L2 error\n",
    "relative_l2_error = torch.mean(torch.norm(all_predictions_denorm - final_time_ground_truth_tensor, dim=1) / \n",
    "                                torch.norm(final_time_ground_truth_tensor, dim=1)) * 100\n",
    "\n",
    "print(f\"Relative L2 Error: {relative_l2_error:.4f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
