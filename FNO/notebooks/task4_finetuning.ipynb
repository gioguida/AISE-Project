{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8246c29",
   "metadata": {},
   "source": [
    "# Task 4 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143d3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fca56fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FNO_bn import FNO1d_bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2310d5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4684a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giugl\\AppData\\Local\\Temp\\ipykernel_14608\\627949701.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  fno.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FNO1d_bn(\n",
       "  (linear_p): Linear(in_features=3, out_features=64, bias=True)\n",
       "  (spect1): SpectralConv1d()\n",
       "  (spect2): SpectralConv1d()\n",
       "  (spect3): SpectralConv1d()\n",
       "  (lin0): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  (lin1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  (lin2): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "  (batch_norm1): FILM(\n",
       "    (inp2scale): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (inp2bias): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (batch_norm2): FILM(\n",
       "    (inp2scale): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (inp2bias): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (batch_norm3): FILM(\n",
       "    (inp2scale): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (inp2bias): Linear(in_features=1, out_features=64, bias=True)\n",
       "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear_q): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FNO_bn import FNO1d_bn\n",
    "# import trained model\n",
    "model_path = \"fno1d_bn_model.pth\"\n",
    "fno = FNO1d_bn(modes=16, width=64)\n",
    "fno.load_state_dict(torch.load(model_path))\n",
    "fno.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f1d8573",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638b580b",
   "metadata": {},
   "source": [
    "## Zero shot test on unknown distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d51f7ba",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74bec0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 128\n",
      "Input shape: torch.Size([128, 128, 2])\n",
      "Time values: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Load test data at resolution 128\n",
    "test_data_raw = np.load(\"data/data_test_unknown_128.npy\")\n",
    "spatial_resolution = 128\n",
    "\n",
    "# Extract initial conditions (t=0) and final time solutions (t=1)\n",
    "initial_conditions = test_data_raw[:, 0, :]  # Shape: (n_samples, 128)\n",
    "final_time_ground_truth = test_data_raw[:, -1, :]  # Shape: (n_samples, 128)\n",
    "\n",
    "# Prepare initial conditions with grid coordinates\n",
    "grid = torch.linspace(0, 1, spatial_resolution, dtype=torch.float32).reshape(spatial_resolution, 1)\n",
    "initial_conditions_tensor = torch.from_numpy(initial_conditions).type(torch.float32).reshape(-1, spatial_resolution, 1)\n",
    "\n",
    "# Add grid coordinates to each sample\n",
    "initial_conditions_with_grid = torch.cat([initial_conditions_tensor, grid.repeat(initial_conditions_tensor.shape[0], 1, 1)], dim=-1)\n",
    "\n",
    "# Normalize initial conditions\n",
    "mean = 0.018484\n",
    "std = 0.685405\n",
    "initial_conditions_with_grid[:, :, 0] = (initial_conditions_with_grid[:, :, 0] - mean) / std\n",
    "\n",
    "# Prepare time tensor (full time span from 0 to 1)\n",
    "time_full = torch.ones(initial_conditions_with_grid.shape[0], dtype=torch.float32) * 1.0\n",
    "\n",
    "# Move to device\n",
    "initial_conditions_with_grid = initial_conditions_with_grid.to(device)\n",
    "time_full = time_full.to(device)\n",
    "\n",
    "print(f\"Test samples: {initial_conditions_with_grid.shape[0]}\")\n",
    "print(f\"Input shape: {initial_conditions_with_grid.shape}\")\n",
    "print(f\"Time values: {time_full[0].item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30017a",
   "metadata": {},
   "source": [
    "### Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1722dbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 Error: 12.5500%\n"
     ]
    }
   ],
   "source": [
    "# Test the model: predict from t=0 to t=1\n",
    "batch_size_test = 20\n",
    "n_test_samples = initial_conditions_with_grid.shape[0]\n",
    "\n",
    "all_predictions = []\n",
    "fno.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, n_test_samples, batch_size_test):\n",
    "        batch_end = min(i + batch_size_test, n_test_samples)\n",
    "        batch_input = initial_conditions_with_grid[i:batch_end]\n",
    "        batch_time = time_full[i:batch_end]\n",
    "        \n",
    "        # Predict final time\n",
    "        predictions = fno(batch_input, batch_time).squeeze(-1)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "all_predictions_denorm = all_predictions * std + mean\n",
    "\n",
    "# Convert ground truth to tensor\n",
    "final_time_ground_truth_tensor = torch.from_numpy(final_time_ground_truth).type(torch.float32).to(device)\n",
    "\n",
    "# Calculate relative L2 error\n",
    "relative_l2_error_zero_shot = torch.mean(torch.norm(all_predictions_denorm - final_time_ground_truth_tensor, dim=1) / \n",
    "                                torch.norm(final_time_ground_truth_tensor, dim=1)) * 100\n",
    "\n",
    "print(f\"Relative L2 Error: {relative_l2_error_zero_shot:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d3d67a",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94ed7d",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e0d1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (32, 5, 128)\n",
      "Training data mean: -0.034377\n",
      "Training data std: 0.358928\n",
      "Training data min: -1.549701\n",
      "Training data max: 1.362039\n"
     ]
    }
   ],
   "source": [
    "# Check actual data statistics\n",
    "train_data = np.load(\"data/data_finetune_train_unknown_128.npy\")\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training data mean: {train_data.mean():.6f}\")\n",
    "print(f\"Training data std: {train_data.std():.6f}\")\n",
    "print(f\"Training data min: {train_data.min():.6f}\")\n",
    "print(f\"Training data max: {train_data.max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d58e135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDEDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 which=\"training\",\n",
    "                 training_samples = 32,\n",
    "                 resolution = 128,\n",
    "                 device='cpu'):\n",
    "\n",
    "        self.resolution = resolution\n",
    "        self.device = device\n",
    "        self.data = np.load(f\"data/data_finetune_train_unknown_{resolution}.npy\")\n",
    "\n",
    "        self.T = 5\n",
    "        # Precompute all possible (t_initial, t_final) pairs within the specified range.\n",
    "        self.time_pairs = [(i, j) for i in range(0, self.T) for j in range(i + 1, self.T)]\n",
    "        self.len_times  = len(self.time_pairs)\n",
    "\n",
    "        # Total samples available in the dataset\n",
    "        total_samples = self.data.shape[0]\n",
    "        self.n_val = 32\n",
    "        self.n_test = 32\n",
    "\n",
    "        if which == \"training\":\n",
    "            self.length = training_samples * self.len_times\n",
    "            self.start_sample = 0\n",
    "        elif which == \"validation\":\n",
    "            self.length = self.n_val * self.len_times\n",
    "            self.start_sample = total_samples - self.n_val - self.n_test\n",
    "        elif which == \"test\":\n",
    "            self.length = self.n_test * self.len_times\n",
    "            self.start_sample = total_samples - self.n_test\n",
    "\n",
    "        # self.mean = -0.034377\n",
    "        # self.std  = 0.358928\n",
    "\n",
    "        self.mean = 0.018484\n",
    "        self.std  = 0.685405\n",
    "        \n",
    "        # Pre-create grid to avoid recreating it each time\n",
    "        self.grid = torch.linspace(0, 1, 128, dtype=torch.float32).reshape(128, 1).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_idx = self.start_sample + index // self.len_times\n",
    "        time_pair_idx = index % self.len_times\n",
    "        t_inp, t_out = self.time_pairs[time_pair_idx]\n",
    "        time = torch.tensor((t_out - t_inp)/4. + float(np.random.rand(1)[0]/10**6), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        inputs = torch.from_numpy(self.data[sample_idx, t_inp]).type(torch.float32).reshape(128, 1).to(self.device)\n",
    "        inputs = (inputs - self.mean)/self.std #Normalize\n",
    "        \n",
    "        # Add grid coordinates (already on correct device and dtype)\n",
    "        inputs = torch.cat((inputs, self.grid), dim=-1)  # (128, 2)\n",
    "\n",
    "        outputs = torch.from_numpy(self.data[sample_idx, t_out]).type(torch.float32).reshape(128).to(self.device)\n",
    "        outputs = (outputs - self.mean)/self.std #Normalize\n",
    "\n",
    "        return time, inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c241d",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2387a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 32 # Number of TRAJECTORIES for training\n",
    "batch_size = 16\n",
    "\n",
    "training_set = DataLoader(PDEDataset(\"training\", n_train, device=device), batch_size=batch_size, shuffle=True)\n",
    "testing_set = DataLoader(PDEDataset(\"validation\", device=device), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "\n",
    "# copy the pre-trained model\n",
    "fno_finetune = copy.deepcopy(fno).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(fno_finetune.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',           # Minimize validation loss\n",
    "    factor=0.5,          # Multiply LR by 0.5\n",
    "    patience=5,          # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6         # Don't go below this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f79e89",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "380eadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 0  ######### Train Loss: 0.03990051739383489  ######### Relative L2 Test Norm: 30.442450523376465\n",
      "######### Epoch: 1  ######### Train Loss: 0.014307809830643236  ######### Relative L2 Test Norm: 14.91901535987854\n",
      "######### Epoch: 2  ######### Train Loss: 0.007367103558499366  ######### Relative L2 Test Norm: 14.380645751953125\n",
      "######### Epoch: 3  ######### Train Loss: 0.005318381142569706  ######### Relative L2 Test Norm: 14.4850914478302\n",
      "######### Epoch: 4  ######### Train Loss: 0.004313120321603492  ######### Relative L2 Test Norm: 10.085830330848694\n",
      "######### Epoch: 5  ######### Train Loss: 0.002903283847263083  ######### Relative L2 Test Norm: 7.209062027931213\n",
      "######### Epoch: 6  ######### Train Loss: 0.003120471228612587  ######### Relative L2 Test Norm: 10.382948684692384\n",
      "######### Epoch: 7  ######### Train Loss: 0.002937784866662696  ######### Relative L2 Test Norm: 11.992182946205139\n",
      "######### Epoch: 8  ######### Train Loss: 0.003220819536363706  ######### Relative L2 Test Norm: 11.56329596042633\n",
      "######### Epoch: 9  ######### Train Loss: 0.0031804567901417614  ######### Relative L2 Test Norm: 11.568990516662598\n",
      "######### Epoch: 10  ######### Train Loss: 0.0025997444899985567  ######### Relative L2 Test Norm: 12.045565223693847\n",
      "######### Epoch: 11  ######### Train Loss: 0.0030379462346900254  ######### Relative L2 Test Norm: 8.815921640396118\n",
      "######### Epoch: 12  ######### Train Loss: 0.0016537907387828455  ######### Relative L2 Test Norm: 7.21058703660965\n",
      "######### Epoch: 13  ######### Train Loss: 0.0019874392717611046  ######### Relative L2 Test Norm: 8.778361606597901\n",
      "######### Epoch: 14  ######### Train Loss: 0.0020409964490681887  ######### Relative L2 Test Norm: 6.7077676653862\n",
      "######### Epoch: 15  ######### Train Loss: 0.0018747210444416851  ######### Relative L2 Test Norm: 7.005805218219757\n",
      "######### Epoch: 16  ######### Train Loss: 0.0019413021625950932  ######### Relative L2 Test Norm: 8.524245262145996\n",
      "######### Epoch: 17  ######### Train Loss: 0.0016261749638943  ######### Relative L2 Test Norm: 6.687601232528687\n",
      "######### Epoch: 18  ######### Train Loss: 0.0015980396943632512  ######### Relative L2 Test Norm: 6.799867904186248\n",
      "######### Epoch: 19  ######### Train Loss: 0.0017274401267059146  ######### Relative L2 Test Norm: 8.585613894462586\n",
      "######### Epoch: 20  ######### Train Loss: 0.0017927645094459876  ######### Relative L2 Test Norm: 9.29733407497406\n",
      "######### Epoch: 21  ######### Train Loss: 0.002600494839134626  ######### Relative L2 Test Norm: 8.935086393356324\n",
      "######### Epoch: 22  ######### Train Loss: 0.002558190008858219  ######### Relative L2 Test Norm: 10.42866485118866\n",
      "######### Epoch: 23  ######### Train Loss: 0.0022251339629292487  ######### Relative L2 Test Norm: 12.038837051391601\n",
      "######### Epoch: 24  ######### Train Loss: 0.001963358049397357  ######### Relative L2 Test Norm: 6.0525516152381895\n",
      "######### Epoch: 25  ######### Train Loss: 0.0012752772599924356  ######### Relative L2 Test Norm: 4.561413061618805\n",
      "######### Epoch: 26  ######### Train Loss: 0.0012444431107724086  ######### Relative L2 Test Norm: 4.1974149465560915\n",
      "######### Epoch: 27  ######### Train Loss: 0.0010560088616330177  ######### Relative L2 Test Norm: 4.627327156066895\n",
      "######### Epoch: 28  ######### Train Loss: 0.0013639314274769276  ######### Relative L2 Test Norm: 7.677488827705384\n",
      "######### Epoch: 29  ######### Train Loss: 0.001494615460978821  ######### Relative L2 Test Norm: 7.143594586849213\n"
     ]
    }
   ],
   "source": [
    "# Define the error function\n",
    "def relative_l2_error(pred, true):\n",
    "    diff_norm = torch.norm(pred - true, p=2, dim=1)\n",
    "    true_norm = torch.norm(true, p=2, dim=1)\n",
    "    return torch.mean(diff_norm / true_norm) * 100\n",
    "\n",
    "l = nn.MSELoss()  \n",
    "freq_print = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    fno_finetune.train()\n",
    "    train_mse = 0.0\n",
    "    for step, (time_batch, input_batch, output_batch) in enumerate(training_set):\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = fno_finetune(input_batch, time_batch).squeeze(-1)\n",
    "        loss_f = l(output_pred_batch, output_batch)\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "\n",
    "    # Validation with CORRECT L2 error\n",
    "    with torch.no_grad():\n",
    "        fno_finetune.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for step, (time_batch, input_batch, output_batch) in enumerate(testing_set):\n",
    "            output_pred_batch = fno_finetune(input_batch, time_batch).squeeze(-1)\n",
    "            # Use the correct relative L2 error\n",
    "            loss_f = relative_l2_error(output_pred_batch, output_batch)\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "    scheduler.step(test_relative_l2)\n",
    "\n",
    "\n",
    "    if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L2 Test Norm:\", test_relative_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946613a4",
   "metadata": {},
   "source": [
    "### Test finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3233d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 Error: 10.4411%\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "fno_finetune.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, n_test_samples, batch_size_test):\n",
    "        batch_end = min(i + batch_size_test, n_test_samples)\n",
    "        batch_input = initial_conditions_with_grid[i:batch_end]\n",
    "        batch_time = time_full[i:batch_end]\n",
    "        \n",
    "        # Predict final time\n",
    "        predictions = fno_finetune(batch_input, batch_time).squeeze(-1)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "all_predictions_denorm = all_predictions * std + mean\n",
    "\n",
    "# Convert ground truth to tensor\n",
    "final_time_ground_truth_tensor = torch.from_numpy(final_time_ground_truth).type(torch.float32).to(device)\n",
    "\n",
    "# Calculate relative L2 error\n",
    "relative_l2_error_finetuned = torch.mean(torch.norm(all_predictions_denorm - final_time_ground_truth_tensor, dim=1) / \n",
    "                                torch.norm(final_time_ground_truth_tensor, dim=1)) * 100\n",
    "\n",
    "print(f\"Relative L2 Error: {relative_l2_error_finetuned:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e54f6",
   "metadata": {},
   "source": [
    "## Bonus: Train new model on unknown distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92014d07",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d525de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 32 # Number of TRAJECTORIES for training\n",
    "batch_size = 16\n",
    "\n",
    "training_set = DataLoader(PDEDataset(\"training\", n_train, device=device), batch_size=batch_size, shuffle=True)\n",
    "testing_set = DataLoader(PDEDataset(\"validation\", device=device), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "step_size = 10\n",
    "gamma = 0.5\n",
    "\n",
    "modes = 16\n",
    "width = 64\n",
    "fno_new = FNO1d_bn(modes, width).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(fno_new.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min',           # Minimize validation loss\n",
    "    factor=0.5,          # Multiply LR by 0.5\n",
    "    patience=5,          # Wait 5 epochs before reducing\n",
    "    min_lr=1e-6         # Don't go below this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bea37e",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8673c818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### Epoch: 0  ######### Train Loss: 0.07147261807695031  ######### Relative L2 Test Norm: 31.054347133636476\n",
      "######### Epoch: 1  ######### Train Loss: 0.025299772527068852  ######### Relative L2 Test Norm: 21.186342763900758\n",
      "######### Epoch: 2  ######### Train Loss: 0.015365667385049164  ######### Relative L2 Test Norm: 16.70710678100586\n",
      "######### Epoch: 3  ######### Train Loss: 0.013853256381116808  ######### Relative L2 Test Norm: 15.826114940643311\n",
      "######### Epoch: 4  ######### Train Loss: 0.012427917285822331  ######### Relative L2 Test Norm: 14.757332372665406\n",
      "######### Epoch: 5  ######### Train Loss: 0.00663308686343953  ######### Relative L2 Test Norm: 15.404463958740234\n",
      "######### Epoch: 6  ######### Train Loss: 0.01010068696923554  ######### Relative L2 Test Norm: 12.60625729560852\n",
      "######### Epoch: 7  ######### Train Loss: 0.01023712222231552  ######### Relative L2 Test Norm: 22.39613308906555\n",
      "######### Epoch: 8  ######### Train Loss: 0.013925889716483652  ######### Relative L2 Test Norm: 21.360390663146973\n",
      "######### Epoch: 9  ######### Train Loss: 0.006684875953942538  ######### Relative L2 Test Norm: 11.648041462898254\n",
      "######### Epoch: 10  ######### Train Loss: 0.008495796076022088  ######### Relative L2 Test Norm: 16.52197947502136\n",
      "######### Epoch: 11  ######### Train Loss: 0.008652480610180646  ######### Relative L2 Test Norm: 15.872046566009521\n",
      "######### Epoch: 12  ######### Train Loss: 0.00785220330581069  ######### Relative L2 Test Norm: 11.9397784948349\n",
      "######### Epoch: 13  ######### Train Loss: 0.006400299351662397  ######### Relative L2 Test Norm: 11.68724021911621\n",
      "######### Epoch: 14  ######### Train Loss: 0.005778989754617214  ######### Relative L2 Test Norm: 13.850112628936767\n",
      "######### Epoch: 15  ######### Train Loss: 0.005174135102424771  ######### Relative L2 Test Norm: 13.732929182052612\n",
      "######### Epoch: 16  ######### Train Loss: 0.008262547146296128  ######### Relative L2 Test Norm: 11.014518570899963\n",
      "######### Epoch: 17  ######### Train Loss: 0.0054164770932402465  ######### Relative L2 Test Norm: 14.178586483001709\n",
      "######### Epoch: 18  ######### Train Loss: 0.00812541478080675  ######### Relative L2 Test Norm: 13.8367418050766\n",
      "######### Epoch: 19  ######### Train Loss: 0.005620690633077174  ######### Relative L2 Test Norm: 8.147681736946106\n",
      "######### Epoch: 20  ######### Train Loss: 0.0053237976913806054  ######### Relative L2 Test Norm: 10.876265072822571\n",
      "######### Epoch: 21  ######### Train Loss: 0.006327536341268569  ######### Relative L2 Test Norm: 11.17934808731079\n",
      "######### Epoch: 22  ######### Train Loss: 0.005406082537956536  ######### Relative L2 Test Norm: 9.381106233596801\n",
      "######### Epoch: 23  ######### Train Loss: 0.007315775519236923  ######### Relative L2 Test Norm: 9.406940722465515\n",
      "######### Epoch: 24  ######### Train Loss: 0.005049297073855996  ######### Relative L2 Test Norm: 8.913246679306031\n",
      "######### Epoch: 25  ######### Train Loss: 0.003875634085852653  ######### Relative L2 Test Norm: 9.5069020986557\n",
      "######### Epoch: 26  ######### Train Loss: 0.0026020784542197363  ######### Relative L2 Test Norm: 6.528047561645508\n",
      "######### Epoch: 27  ######### Train Loss: 0.003558244602754712  ######### Relative L2 Test Norm: 6.23124930858612\n",
      "######### Epoch: 28  ######### Train Loss: 0.0033580760355107486  ######### Relative L2 Test Norm: 7.080955266952515\n",
      "######### Epoch: 29  ######### Train Loss: 0.004050948622170836  ######### Relative L2 Test Norm: 7.20185387134552\n",
      "######### Epoch: 30  ######### Train Loss: 0.0032949049025774004  ######### Relative L2 Test Norm: 6.982537078857422\n",
      "######### Epoch: 31  ######### Train Loss: 0.003676363610429689  ######### Relative L2 Test Norm: 7.146875524520874\n",
      "######### Epoch: 32  ######### Train Loss: 0.004685932790744118  ######### Relative L2 Test Norm: 10.336594390869141\n",
      "######### Epoch: 33  ######### Train Loss: 0.0043241006962489335  ######### Relative L2 Test Norm: 8.126969861984254\n",
      "######### Epoch: 34  ######### Train Loss: 0.0033240758231841026  ######### Relative L2 Test Norm: 6.498245406150818\n",
      "######### Epoch: 35  ######### Train Loss: 0.002788312721531838  ######### Relative L2 Test Norm: 5.975594544410706\n",
      "######### Epoch: 36  ######### Train Loss: 0.003044726891675964  ######### Relative L2 Test Norm: 7.161124277114868\n",
      "######### Epoch: 37  ######### Train Loss: 0.0025184089376125486  ######### Relative L2 Test Norm: 6.010703372955322\n",
      "######### Epoch: 38  ######### Train Loss: 0.003751756955171004  ######### Relative L2 Test Norm: 5.842614388465881\n",
      "######### Epoch: 39  ######### Train Loss: 0.0028644241974689066  ######### Relative L2 Test Norm: 6.21924991607666\n",
      "######### Epoch: 40  ######### Train Loss: 0.002975212983437814  ######### Relative L2 Test Norm: 5.866051721572876\n",
      "######### Epoch: 41  ######### Train Loss: 0.004618434945587069  ######### Relative L2 Test Norm: 6.483030366897583\n",
      "######### Epoch: 42  ######### Train Loss: 0.0041132400918286295  ######### Relative L2 Test Norm: 7.09988043308258\n",
      "######### Epoch: 43  ######### Train Loss: 0.0035382004047278316  ######### Relative L2 Test Norm: 5.890193438529968\n",
      "######### Epoch: 44  ######### Train Loss: 0.0030404488104977647  ######### Relative L2 Test Norm: 6.207019245624542\n",
      "######### Epoch: 45  ######### Train Loss: 0.003005287647829391  ######### Relative L2 Test Norm: 5.353485786914826\n",
      "######### Epoch: 46  ######### Train Loss: 0.002013125405937899  ######### Relative L2 Test Norm: 5.760658216476441\n",
      "######### Epoch: 47  ######### Train Loss: 0.0028355376271065325  ######### Relative L2 Test Norm: 5.689616179466247\n",
      "######### Epoch: 48  ######### Train Loss: 0.0021763090218883006  ######### Relative L2 Test Norm: 5.56376461982727\n",
      "######### Epoch: 49  ######### Train Loss: 0.0027366686932509764  ######### Relative L2 Test Norm: 5.656410074234008\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    fno_new.train()\n",
    "    train_mse = 0.0\n",
    "    for step, (time_batch, input_batch, output_batch) in enumerate(training_set):\n",
    "        optimizer.zero_grad()\n",
    "        output_pred_batch = fno_new(input_batch, time_batch).squeeze(-1)\n",
    "        loss_f = l(output_pred_batch, output_batch)\n",
    "        loss_f.backward()\n",
    "        optimizer.step()\n",
    "        train_mse += loss_f.item()\n",
    "    train_mse /= len(training_set)\n",
    "\n",
    "\n",
    "    # Validation with CORRECT L2 error\n",
    "    with torch.no_grad():\n",
    "        fno_new.eval()\n",
    "        test_relative_l2 = 0.0\n",
    "        for step, (time_batch, input_batch, output_batch) in enumerate(testing_set):\n",
    "            output_pred_batch = fno_new(input_batch, time_batch).squeeze(-1)\n",
    "            # Use the correct relative L2 error\n",
    "            loss_f = relative_l2_error(output_pred_batch, output_batch)\n",
    "            test_relative_l2 += loss_f.item()\n",
    "        test_relative_l2 /= len(testing_set)\n",
    "    scheduler.step(test_relative_l2)\n",
    "\n",
    "\n",
    "    if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L2 Test Norm:\", test_relative_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7b1c6",
   "metadata": {},
   "source": [
    "### Test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e15b752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative L2 Error: 14.0566%\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "fno_new.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, n_test_samples, batch_size_test):\n",
    "        batch_end = min(i + batch_size_test, n_test_samples)\n",
    "        batch_input = initial_conditions_with_grid[i:batch_end]\n",
    "        batch_time = time_full[i:batch_end]\n",
    "        \n",
    "        # Predict final time\n",
    "        predictions = fno_new(batch_input, batch_time).squeeze(-1)\n",
    "        all_predictions.append(predictions)\n",
    "\n",
    "# Concatenate all predictions\n",
    "all_predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "# Denormalize predictions\n",
    "all_predictions_denorm = all_predictions * std + mean\n",
    "\n",
    "# Convert ground truth to tensor\n",
    "final_time_ground_truth_tensor = torch.from_numpy(final_time_ground_truth).type(torch.float32).to(device)\n",
    "\n",
    "# Calculate relative L2 error\n",
    "relative_l2_error_new = torch.mean(torch.norm(all_predictions_denorm - final_time_ground_truth_tensor, dim=1) / \n",
    "                                torch.norm(final_time_ground_truth_tensor, dim=1)) * 100\n",
    "\n",
    "print(f\"Relative L2 Error: {relative_l2_error_new:.4f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
